to train word2vec on some field of data.
use DumpForWord2Vec

// to train word2vec on dumps
./word2vec -train /media/i3/7233c927-5a17-4fe9-bedd-784c6f7e151d/content_dump_word2vec_train_wapo_v3.txt -output wapov3.bin -cbow 1 -size 500 -window 10 -negative 10 -hs 0 -sample 1e-5 -threads 40 -binary 1 -iter 3 -min-count 10

// dump top terms to text file (remove printf)

// to construct document embeddings from word2vec:


----------------------------------------------------
Contents array in JSON

1. type:"kicker" denotes the type of article
we reject


{"content": "Letters to the Editor", "mime": "text/plain", "type": "kicker"}

{"type": "kicker", "storyType": "Opinion"}
{"content": "The Post's View", "mime": "text/plain", "type": "kicker"}

2. type:title

3. type:byline

4. type: date

5. type: deck //optional "opinions" dont have

6. type: image

7 onwards.. sanitized_html

// for opinion types.. sanitized_html(content starts from 5 onwards generally)


-----------------------------------------------------------
 Method 4 (htwsaar4) = title and body query+  a date filter is applied to allow only documents published on or before
the publication date of the input document.
 our method htwsaar4
looses on topic 818. The document about dietary recommendations related to
cholesterol contains many general terms, leading to a diluted query.

performance: 0.4619


-----------------------------------------------------------
Time filtering
We submitted runs that pruned all documents published after the query document. Documents missing a
publication time were considered to be older than any others and were therefore included. Although this
was not part of the final ruleset, it actually significantly improved results, at least in terms of precision!
(umass rm achieves lower performance without time filtering: NDCG@5=3.556 vs. NCDG@5=0.4157).
In future submissions to TREC News, we hope to explore this relationship between time and relevance
more deeply.
---------------------------
BM25(k1=1.2,b=0.75)
------------------------------
boosted tfidf
ndcg_cut_5              all     0.3420
top terms from title and content

dont show duplicate docs
---------------------------------
boosted tfidf
ndcg_cut_5              all     0.3855
top terms from title and content (1000)
boosted
dont show duplicate docs
------------------------------------


ndcg_cut_5              all     0.3802
ndcg_cut_10             all     0.3848
ndcg_cut_15             all     0.3945
ndcg_cut_20             all     0.3964
ndcg_cut_30             all     0.3977
ndcg_cut_100            all     0.4408
ndcg_cut_200            all     0.4360
ndcg_cut_500            all     0.4347
ndcg_cut_1000           all     0.4347

boosted tfidf
top terms from content (25)
boosted
dont show duplicate docs
DateFilter
----------------------------------------------

ndcg_cut_5              all     0.3345
ndcg_cut_10             all     0.3685
ndcg_cut_15             all     0.3784
ndcg_cut_20             all     0.3884
ndcg_cut_30             all     0.4010
ndcg_cut_100            all     0.4584
ndcg_cut_200            all     0.4537
ndcg_cut_500            all     0.4525
ndcg_cut_1000           all     0.4525

boosted tfidf
top terms from content (30)
boosted
dont show duplicate docs
no date filter
---------------------------------------------
ndcg_cut_5              all     0.3978
ndcg_cut_10             all     0.4015
ndcg_cut_15             all     0.4073
ndcg_cut_20             all     0.4099
ndcg_cut_30             all     0.4193
ndcg_cut_100            all     0.4593
ndcg_cut_200            all     0.4545
ndcg_cut_500            all     0.4532
ndcg_cut_1000           all     0.4532

boosted tfidf
top terms from content (30) -- search only in content part
top terms from title(30) -- search only in title part
boosted
dont show duplicate docs
no date filter
-------------------------------------------------
mlt-title-content.txt
ndcg_cut_5              all     0.3997
ndcg_cut_10             all     0.4085
ndcg_cut_15             all     0.4140
ndcg_cut_20             all     0.4205
ndcg_cut_30             all     0.4265
ndcg_cut_100            all     0.4679
ndcg_cut_200            all     0.4631
ndcg_cut_500            all     0.4618
ndcg_cut_1000           all     0.4618

same as prev plus terms from title are searched also in content
----------------------
res2020-01-24_22-22-08.txt
ndcg_cut_5              all     0.3955
ndcg_cut_10             all     0.4068
ndcg_cut_15             all     0.4126
ndcg_cut_20             all     0.4176
ndcg_cut_30             all     0.4249
ndcg_cut_100            all     0.4662
ndcg_cut_200            all     0.4614
ndcg_cut_500            all     0.4601
ndcg_cut_1000           all     0.4601

author match condition added
----------------------------------
res2020-01-24_22-42-56.txt
ndcg_cut_5              all     0.3975
ndcg_cut_10             all     0.4081
ndcg_cut_15             all     0.4146
ndcg_cut_20             all     0.4198
ndcg_cut_30             all     0.4239
ndcg_cut_100            all     0.4621
ndcg_cut_200            all     0.4574
ndcg_cut_500            all     0.4561
ndcg_cut_1000           all     0.4561

300  title words
700 text words
-----------------------------------
mlt_allfilter_50content_30title.txt
same 50 content 30 title
ndcg_cut_5              all     0.4060
ndcg_cut_10             all     0.4095
ndcg_cut_15             all     0.4181
ndcg_cut_20             all     0.4236
ndcg_cut_30             all     0.4297
ndcg_cut_100            all     0.4682
ndcg_cut_200            all     0.4634
ndcg_cut_500            all     0.4621
ndcg_cut_1000           all     0.4621
------------------------------------------
mlt_allfilter_title30_content100
ndcg_cut_5              all     0.3961
ndcg_cut_10             all     0.4073
ndcg_cut_15             all     0.4148
ndcg_cut_20             all     0.4184
ndcg_cut_30             all     0.4243
ndcg_cut_100            all     0.4617
ndcg_cut_200            all     0.4569
ndcg_cut_500            all     0.4557
ndcg_cut_1000           all     0.4557

-----------------------------------------
mlt_content30_title10_allfilter.txt
ndcg_cut_5              all     0.4060
ndcg_cut_10             all     0.4095
ndcg_cut_15             all     0.4181
ndcg_cut_20             all     0.4236
ndcg_cut_30             all     0.4297
ndcg_cut_100            all     0.4682
ndcg_cut_200            all     0.4634
ndcg_cut_500            all     0.4621
ndcg_cut_1000           all     0.4621
---------------------------------------
mlt_allfilter_content20_title10
ndcg_cut_5              all     0.4031
ndcg_cut_10             all     0.4134
ndcg_cut_15             all     0.4090
ndcg_cut_20             all     0.4179
ndcg_cut_30             all     0.4234
ndcg_cut_100            all     0.4643
ndcg_cut_200            all     0.4595
ndcg_cut_500            all     0.4582
ndcg_cut_1000           all     0.4582
-----------------------------------------
contetnt 40 title 10
ndcg_cut_5              all     0.4004
ndcg_cut_10             all     0.4099
ndcg_cut_15             all     0.4140
ndcg_cut_20             all     0.4188
ndcg_cut_30             all     0.4263
ndcg_cut_100            all     0.4679
ndcg_cut_200            all     0.4631
ndcg_cut_500            all     0.4618
ndcg_cut_1000           all     0.4618
-------------------------------------------
content 30 title 20
ndcg_cut_5              all     0.3997
ndcg_cut_10             all     0.4085
ndcg_cut_15             all     0.4140
ndcg_cut_20             all     0.4205
ndcg_cut_30             all     0.4265
ndcg_cut_100            all     0.4679
ndcg_cut_200            all     0.4631
ndcg_cut_500            all     0.4618
ndcg_cut_1000           all     0.4618
-------------------------------------------
content 30 title 5
ndcg_cut_5              all     0.3970
ndcg_cut_10             all     0.4107
ndcg_cut_15             all     0.4113
ndcg_cut_20             all     0.4136
ndcg_cut_30             all     0.4218
ndcg_cut_100            all     0.4626
ndcg_cut_200            all     0.4579
ndcg_cut_500            all     0.4567
ndcg_cut_1000           all     0.4567
--------------
// result 0 in 
423034
bd1e6cc8d7525fec36a717be45638bf4

title: Alan Gross returns to the United States, is greeted by an image of Che Guevara

content:
Alan Gross, the U.S. contractor released Wednesday after five years of captivity in Cuba, 
was greeted with a, well, surprising art choice while preparing to make a statement on U.S. 
soil this afternoon.Visible in the background of a widely circulated photograph of Gross, 
his lawyer Scott Gilbert and spokesperson Jill Zuckman: An image of Che Guevara, 
the divisive revolutionary figure.“Why is there a Che Guevara pic in background?” 
National Review Online’s Andrew Johnson tweeted.Others responded with 
sympathy: “Poor guy gets to his lawyer’s office and has to look at yet another Che glamour shot,
” Andrew Grossman tweeted.Zuckman told The Post via e-mail that the photograph was
 taken at the Gilbert LLP law firm.The print, from a distance, 
 appears to be the same image that’s often emblazoned on T-shirts, posters and other memorabilia. 
 That iconic image is based on a photograph by Alberto “Korda” Diaz Gutierrez; 
 we have more on the history of the image here.Guevara was a leader in the Castro brothers’ 
 revolution to overthrow the Batista dictatorship in Cuba and replace it with a communist government.
  As the Huffington Post noted: “Cuban-Americans generally revile him for his role in 
  establishing the island’s Communist dictatorship.”In his remarks, Gross made it clear 
  that he did not blame the Cuban people for his imprisonment:“In no way are they responsible for 
  the ordeal to which my family and I have been subjected,” Gross said. “To me, Cubanos — or at least,
   most of them — are incredibly kind, generous and 
talented. It pains me to see them treated so unjustly as a consequence of two governments’ 
mutually belligerent policies.”


-----------------------------------------------------------------------------------------------------
params vs NDCG
content title score
30	100	0.3997
30  10  0.40
30	50  0.3997
------------------------------------------------------------
-------------------------------------------------------------
40 content 10 title works best in 

ndcg_cut_5              all     0.4004
ndcg_cut_5              825     1.0000
ndcg_cut_5              821     0.0740
ndcg_cut_5              817     0.0000
ndcg_cut_5              814     0.7259
ndcg_cut_5              805     0.7814
ndcg_cut_5              442     0.0891
ndcg_cut_5              439     0.0000
ndcg_cut_5              422     0.0000
ndcg_cut_5              414     0.1375
ndcg_cut_5              400     0.5560

------------------------------------------------------
Date : last 3 zeros has to be dropped in date format
--------------------------------------------------------
./trec_eval /home/i3/Documents/lucene-class-tutorial/adhoc.fire.en/en.qrels.126-175.2011.txt /home/i3/Documents/FIRE_Adhoc_SearchEngine/mtc1835-prf.res -q

--------------------------------------------------------------
./trec_eval ../bqrels.exp-gains.txt /home/i3/git/Relevance-Model/results/topics.xml-LMJelinek-Mercer0.6-D20-T70-rm3-0.6.res.baseline -m ndcg_cut
----------------------------------------------------------
DOIDA RM3 results

title query only to content
ndcg_cut_5            	all	0.1268
prev+rm3 words added from content
ndcg_cut_5            	all	0.1419


title query only to title
ndcg_cut_5            	all	0.0972
prev+rm3 words added from content
ndcg_cut_5            	all	0.1335

title query to title
0.1265
pick words from title
0.1300

words from content
200: 3931
150: 3928
100: 3936
90: 3932
80: 3958
70: 0.3940
60: 3956
50: 3958
40: 3872
30: 3772
10: 3366

words from title only 
10: 2255
20: 2276
30:

-------
mix hitlist

lambda	title_len	content_len		score		normalized_add
------------------------------------------------------------------------------
0.5		20			50				0.3352		yes
"		"			"				0.3789		no
0.2		20			50				0.37		yes
"		"			"				0.3931		no
"		"			"				0.5259(2019)no		
0.1		"			"				0.3973	*	no
"		"			"				0.3787		yes

----------------------------------------------------------------
this score is for ner table and normal table merge
0.8		-			-				0.3282		yes
(title-content table) 
0.8		-			-				0.3325		not normalized

#we notice not_normalized generally improves performance so we dont normalize

# content + contentner query
ndcg_cut_5            	all	0.3714

# content + contentner query+lambda*content_query_boost+(1-lambda)*contentner_query_boost
ndcg_cut_5            	all	0.3873 lambda=0.8 
ndcg_cut_5            	all	0.3002 lambda=0.2
ndcg_cut_5				all	0.3898	lambda = 0.9
ndcg_cut_5				all 0.3958	lambda = 1

-------------------------------------------------------------
# content + titlener query+lambda*content_query_boost+(1-lambda)*titlener_query_boost
ndcg_cut_5            	all	0.3969 lambda=0.8
ndcg_cut_5            	all	0.3961 lambda=0.6*vecct
ndcg_cut_5            	all	0.3941	lambda = 0.5
ndcg_cut_5            	all	0.5254(2019)*
-------------------------------------------------------
vector+content-nerquery
----------------------------------------------------
lambda*queryTopdocc (1-lambda)*vectortopdoc
*lambda = 0.6 0.4041 (more robust)
lambda = 0.8 0.4007
lambda = 0.4 0.4022
lambda = 0.5 0.4063

*lambda = 0.6 0.5131 (2019 result)
------------------------------------------------------
word2vec only
2018 35 words
0.3175 

-------------------------------------
Qmix
title 30*0.4,mincoccur = 1:  content 50*0.6,minoccur=2 ---> 3894
./results/res2020-07-08_16-48-01.res 

title 20*0.4,mincoccur = 1:  content 50*0.6,minoccur=2 ---> 3894
../results/res2020-07-08_16-43-14.res 


title 20*0.5,mincoccur = 1:  content 50*0.5,minoccur=2 ---> 3909
../results/res2020-07-08_16-55-36.res 

title 20*0.5,mincoccur = 1:  content 80*0.5,minoccur=2 ---> 3909
../results/res2020-07-08_16-55-36.res 



title 20*0.1,mincoccur = 1:  content 80*0.9,minoccur=2 ---> 
./trec_eval ../qrels/bqrels.exp-gains.txt -m ndcg_cut ../results/res2020-07-08_20-00-39.res 
ndcg_cut_5            	all	0.3988


onlycontent-bm25-words=80-2019
../results/res2020-07-08_20-30-48.res 
ndcg_cut_5            	all	0.4860****



-----------------------------------------
docvec-2018-unweightedSum-words=20
./trec_eval ../qrels/bqrels.exp-gains.txt -m ndcg_cut ../results/res2020-07-08_22-41-04.res 
ndcg_cut_5            	all	0.2614****


docvec-2018-unweightedSum-words=25
 ./trec_eval ../qrels/bqrels.exp-gains.txt -m ndcg_cut ../results/res2020-07-08_22-31-36.res 
ndcg_cut_5            	all	0.2736****

